<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SepBIN Details</title>
        <style>
        body {
            font-family: 'Times New Roman', Times, serif;
            font-size: 18px;
            line-height: 1.2;
            text-align: left;
            margin-left: 3.5in;
            margin-right: 3.5in;
        }
        h1, h2, h3 {
            font-family: 'Arial Black', Gadget, sans-serif;
        }
        p {
            text-indent: 2em;
        }
        img {
            display: block;
            margin: 0 auto;
        }
        ol.references-list {
        font-size: 0.8em;
        }
    </style>
</head>

<body>
<h1>Supporting Material for SepBIN </h1>
<p>
    This web page is used as a supplementary web page for our manuscript "SepBIN: Binary Feature Separation for Better Semantic Comparison and Authorship Verification", submitted to IEEE Transactions on Information Forensics & Security.
    The page contains the download link and descriptions of our experimental data, as well as additional technical details about our dataset and evaluations.
</p>

<h2>SepBIN Experimental Data</h2>
    <p>The experimental data of <code>SepBIN</code> can be downloaded by the link
<a href="https://drive.google.com/file/d/1pKcHAbJaoSxMkN4DNM-zWXn7E_EdFDpT/view?usp=sharing">https://drive.google.com/file/d/1pKcHAbJaoSxMkN4DNM-zWXn7E_EdFDpT/view?usp=sharing</a>.
    </p>

    <h3>Directory Description</h3>
    <ul>
        <li>The data/GCJs folder contains processed feature files and ranking pair files for the GCJ-C I, GCJ-C II, GCJ-C++ I, GCJ-C++ II datasets, as described in Section V.A.1.</li>
        <li>The data/obfuscations folder includes processed feature files and ranking pair files for the single and combined obfuscated datasets in Section V.D.1.</li>
        <li>The data/aptmalware folder includes processed feature files and ranking pair files for the APT malware homology detection dataset in Section V.D.2 to V.D.4.</li>
        <li>The data/Github folder includes processed feature files and ranking pair files for the Github dataset in Section B of this web page.</li>
    </ul>

<h2>SepBIN Additional Details</h2>
        <h3>Extended experiments on realistic Github dataset</h3>

        <p>We further introduced the Github data source to evaluate <code>SepBIN</code>'s performance on realistic datasets more comprehensively.
        Specifically, we crawled and compiled C-language programs from the top-star Github pages and utilized the <em>git blame</em> command to
        determine line-by-line authorship. After that, we filtered out programs where more than 30% of the code lines weren't developed by
        its main contributor. The final dataset contains 3,648 binary files originating from 250 Github repositories, involving a
        total of 126 authors. Unlike GCJ, the Github data source lacks precise semantic-level annotations,
        and we constructed it as a single-label downstream dataset for the authorship verification task.
        We randomly selected query binaries with 20 associated candidates: 5 positives from the same developer as the query sample,
        and 15 negatives from other developers. In total, our Github dataset comprises 60,000 binary pairs,
        with 30,000 for finetuning <code>SepBIN</code> and the remaining as the test set.</p>

        <p>We evaluate the fully finetuned <code>SepBIN</code> and make comparisons with the <em>Siamese</em>-based baseline model on the Github dataset.
        For <code>SepBIN</code>, we also conducted multi-objective pretraining on the GCJC dataset to acquire binary semantic-style separation
        capability. Experimental results are shown in Table I. As can be observed, even though the realistic Github dataset
        might have inherent challenges during data collection and annotation processes, such as longer source codes with more complex functionalities,
        interference of co-author code fragments on modeling the programming style of  the main contributor, and the reuse of open-source code or
        libraries, <code>SepBIN</code> still demonstrates clear advantages. The Precision@1 and Recall@5 metrics reach 85.13% and 75.16%, respectively,
        outperforming the baseline model by 8.06 and 6.48 points.</p>

        The dataset collection process and experimental results are detailed in the document download link.<br><br>

        <img src="github_res2.png" width="600"><br>

        <p>Furthermore, similar to the APT malware homology detection experiments, we utilized different proportions of
        finetuning datasets (10%, 50%, 100%) to perform in-depth evaluations. The results are presented in Figure 1.
        It can be seen that under all settings, <code>SepBIN</code> maintains improvements compared to the baseline model.
        The gains in Precision@1 and Recall@5 metrics range from 2.14 to 11.13 points. Overall, the extended experiments
        on the realistic Github dataset further prove that <code>SepBIN</code>'s network architecture and pretraining-finetuning
        mechanism can effectively transfer the feature separation ability, benefiting real-world binary authorship
        verification applications.</p>

        <img src="github_res1.png" width="400"><br>

        <p>Tools for identifying anti-analysis strategies of APT malware samples</h3>
        When evaluating <code>SepBIN</code>'s performance on real-world APT malware homology detection applications, we particularly
        explore its robustness against anti-analysis strategies in Section V.D.3. We mainly leverage the CAPA tool
        provided by the security vendor <em>Mandiant</em> to statistic the adversarial strategies involved in our APT
        malware samples and construct the experimental dataset. CAPA can effectively identify high-level malware
        capabilities through a high-quality rule collection constructed by experienced reverse engineering experts.
        It currently includes 832 detection rules, 87 of which are related to adversarial capabilities, including
        Anti-Debugger (16 rules), Anti-AV (4 rules), Anti-VM (18 rules), Obfuscation (10 rules), Data-encoding (6 rules),
        and Data-encryption (33 rules).</p>

        As an advanced industry malware analysis tool, CAPA has also been widely used in academic research <sup>[5-12]</sup>.
        Researchers often exploit it to perform malware feature extraction and malware dataset ground-truth annotation.
        For example, Joren Vrancken <em>et al.</em> <sup>[6]</sup> took CAPA as a strong baseline for their experiments
        on detecting capabilities in malware binaries. Michael R. Smith <em>et al.</em> <sup>[7]</sup> utilized CAPA as a
        robust automatic executable labeling tool to provide ground-truth malware behavior labels and assist their malware
        detection approach. Andre T. Nguyen <em>et al.</em> <sup>[8]</sup> used CAPA as an expensive and accurate malware capability
        detection tool to provide better results when other cheaper analysis tools perform unsatisfactorily.
        They also used the rule-matching results of CAPA as the ground-truth for their experimental evaluation.
        George David <em>et al.</em> <sup>[9]</sup> and Ethan M. Rudd <em>et al.</em> <sup>[10]</sup> leveraged the
        high-level capability features extracted by CAPA to support malware clustering, detection, and family
        classification tasks. These previous works demonstrate the accuracy and reliability of the CAPA tool for
        malware analysis research from diverse perspectives.

        <p>To conduct more comprehensive investigations, we further consider other tools to extract the adversarial strategies of the
        APT malware samples, and perform comparative analysis with CAPA. We selected three publicly available malware
        analysis tools provided by security vendors or communities, including <em>ThreatBook Cloud Sandbox</em>,
        <em>VirusTotal</em>, and <em>Intezer Analyze</em>. They support malware capability identification by different
        mechanisms, as follows:</p>

        <ul>
            <li><em>ThreatBook Cloud Sandbox </em><sup>[1]</sup> is a Chinese online malware analysis platform.
        It executes malware samples within diverse sandbox environments to extract behavioral data such as API calls,
        network activities, and file operations.</li>
            <li><em>VirusTotal</em> <sup>[2]</sup> is a well-known security analysis community that amalgamates
        various antivirus and security scanning engines. It integrates dynamic behavior analysis results of multiple sandbox
        products and allows users to retrieve analysis reports through the file behavior API <sup>[3]</sup>.<br></li>
            <li><em>Intezer Analyze</em> <sup>[4]</sup> is a cloud-based Israeli malware scanning and analysis platform.
        It identifies the vital functionalities of malware through code gene analysis and builds the assembly code into predefined rules covering the MITER ATT&CK framework.<br></li>
        </ul>

        <p>We demonstrate the performance of CAPA and the above tools on our APT malware dataset through specific cases.
        Tables II to IV show their analysis results on three  malware samples of APT29, Equation, and Darkhotel groups,
        respectively. The adversarial strategies detected by each tool are signed with check marks, along with the number
        of matched rules or identified behaviors (denoted by Na). Tools that did not identify special anti-analysis-related
        attributes are omitted in the table.</p>

        <img src="capa_res_tables.png" width="600"><br>

        <p>By analyzing the above evaluation results, we can observe that the malware anti-analysis technologies
        identified by CAPA are more comprehensive, almost covering the identification results of <em>ThreatBook Cloud Sandbox</em>,
        <em>VirusTotal</em>, and <em>Intezer</em>. And the results of the latter three tools in turn verify the accuracy and
        reliability of CAPA. For example, <em>ThreatBook Cloud Sandboxes</em> identified the presence of Anti-debugger and
        Anti-VM strategies in the sample <em>69232da84dc7d9b2fdf1f1daade6eaae</em> of APT29 through dynamic file execution
        behaviors such as API calls and instruction traces. CAPA completed the corresponding detection based on the rules built
        from unique raw byte sequences or other related features. Although their detection abilities are constructed from different
        perspectives and based on different attributes, their overall conclusions are consistent.
        As <em>Threatbook Cloud Sandbox</em> reveals solid abilities to identify Anti-VM and Anti-Debugger behaviors, we consider
        combining it with CAPA for better exploring the robustness of <code>SepBIN</code> in dealing with highly adversarial real-world malware analysis scenarios.</p>

<h2>References</h2>
<ol class="references-list">
    <li>https://s.threatbook.com</li>
    <li>https://www.virustotal.com</li>
    <li>https://developers.virustotal.com/reference/get-all-behavior-reports-for-a-file</li>
    <li>https://analyze.intezer.com</li>
    <li>Raffa, G., Sgandurra, D., & O’Keeffe, D. (2022, December). Evaluating Anti-Virus Effectiveness in Linux. In 2022 IEEE International Conference on Big Data (Big Data) (pp. 3071-3080). IEEE.</li>
    <li>Vrancken, J., Poll, E., & Verbeek, F. (2022). Detecting Capabilities in Malware Binaries by Searching for Function Calls. Master thesis, Radboud University, 2022.</li>
    <li>Smith, M., Carbajal, A., Domschot, E., Johnson, N. T., Goyal, A., Lamb, C., ... & Zhou, X. (2022). MalGen: Malware Generation with Specific Behaviors to Improve Machine Learning-based Detectors (No. SAND2022-14321). Sandia National Lab.(SNL-NM), Albuquerque, NM (United States); Sandia National Laboratories,, Livermore, CA.</li>
    <li>Nguyen, A. T., Zak, R., Richards, L. E., Fuchs, M., Lu, F., Brandon, R., ... & Holt, J. (2022). Minimizing Compute Costs: When Should We Run More Expensive Malware Analysis?. Conference on Applied Machine Learning in Information Security (CAMLIS).</li>
    <li>George, D., Mauldin, A., Mitchell, J., Mohammed, S., & Slater, R. (2023). Static Malware Family Clustering via Structural and Functional Characteristics. SMU Data Science Review, 7(2), 4.</li>
    <li>Rudd, E. M., Krisiloff, D., Coull, S., Olszewski, D., Raff, E., & Holt, J. (2022). Efficient Malware Analysis Using Metric Embeddings. arXiv preprint arXiv:2212.02663.</li>
    <li>Schultz, N., & Duby, A. (2022, June). Towards a framework for preprocessing analysis of adversarial windows malware. In 2022 10th International Symposium on Digital Forensics and Security (ISDFS) (pp. 1-6). IEEE.</li>
    <li>Fujii, S., Yamagishi, R., & Yamauchi, T. (2022). Survey and Analysis on ATT&CK Mapping Function of Online Sandbox for Understanding and Efficient Using. Journal of Information Processing, 30, 807-821.</li>
</ol>

<p>
Please address correspondence concerning this manuscript and the detail information to the email address: songqg@zgclab.edu.cn.
</p>
</body>
</html>
